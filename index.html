<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Wells Fargo Social Media Analytic Challenge by rachel-feldman</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Wells Fargo Social Media Analytic Challenge</h1>
      <h2 class="project-tagline">Results from Wells Fargo College Analytic Challenge</h2>
      <a href="https://github.com/rachel-feldman/wellsfargosocialmedia" class="btn">View on GitHub</a>
      <a href="https://github.com/rachel-feldman/wellsfargosocialmedia/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/rachel-feldman/wellsfargosocialmedia/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="analyzing-social-media-response" class="anchor" href="#analyzing-social-media-response" aria-hidden="true"><span class="octicon octicon-link"></span></a>Analyzing Social Media Response</h1>

<h3>
<a id="by-rachel-feldman-jeniffer-soto-perez-and-casey-salvador" class="anchor" href="#by-rachel-feldman-jeniffer-soto-perez-and-casey-salvador" aria-hidden="true"><span class="octicon octicon-link"></span></a>BY RACHEL FELDMAN, JENIFFER SOTO-PEREZ, AND CASEY SALVADOR</h3>

<h2>
<a id="about-me" class="anchor" href="#about-me" aria-hidden="true"><span class="octicon octicon-link"></span></a>About Me</h2>

<p><img src="http://i.imgur.com/2Mn2vgqt.jpg" alt="">My name is Rachel Feldman and I am a student at the College of Charleston.  I concentrate in creating and imagining audience experiences of the highest caliber in the arts starting from the first interaction online to the post-event sentiment.  The following project is a part of Dr. Paul Anderson's Data 101 class.  For questions or comments contact me via email at <a href="mailto:feldmanrm@g.cofc.edu">feldmanrm@g.cofc.edu</a></p>

<h2>
<a id="the-competition" class="anchor" href="#the-competition" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Competition</h2>

<p>From the competition website:
Dialogues on social media can provide tremendous insight into the behaviors, desires, pains, and thoughts of consumers. We'd like your help in developing a repeatable process that identifies, classifies, and extracts the underlying drivers of consumer financial conversations and comments in social media data. </p>

<h2>
<a id="preparing-the-data" class="anchor" href="#preparing-the-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Preparing the Data</h2>

<p>In order to pull useful information from the data given, we put the dataset in R studio.  We mined the tweets and posts through R Studio, getting rid of blank spaces, messages that did not tag a particular bank.  Additionally, we spell checked the information.  </p>

<h2>
<a id="approach-and-methodology" class="anchor" href="#approach-and-methodology" aria-hidden="true"><span class="octicon octicon-link"></span></a>Approach and Methodology</h2>

<p>Once we prepared the tweets and posts for analysis, we used an agile methodology so we could test small amounts of data at a time to quicken the process. 
An agile methodology was chosen so we could start to answer smaller questions and work our way up to bigger ones that would lead to bigger and broader results.  Our methods could be repeated and compared for each bank. Through our process, we tested 1000 random samples of the entire data set and then 1000 random samples of only bank A to compare and contrast differences.
Below, find a graphical representation of the analytic and developmental approach taken.  It will show first cleaning the data where the team got rid of any additional room that was taking up space.  The team then tested the data overall to find positive and negative sentiment words as well as high-frequency words.  This gave us the framework to look at the data as a whole before looking into the context of each bank.  This step also allowed for the creation of two separate files for negative and positive posts.<br>
The team indexed each bank to then replicate the tests: positive and negative as well as word frequency. 
The results were then tested against each bank's and the data set as a whole.  Finally, we tried to pull conclusions about what our results may tell us about the bank industry's presence in social media as a whole.</p>

<p><img src="http://i.imgur.com/XI5Eh29.jpg" alt="Method"></p>

<p>Our methodology mirrors the first three steps of the DIKW pyramid.  The pyramid shows data at the bottom, then information, knowledge, and wisdom.  With our approach, we are converting the data into information that means something.  With insights, comparing, and classification methods, we hope to convert the information into knowledge.  This knowledge we hope will be able to predict and make assumptions about the different banks and if supported for a time, later be applied to practices to be wisdom.  </p>

<p><img src="http://www.allthingy.com/wp-content/uploads/2014/07/Wisdom-Knowledge-Information-Data-Pyramid15.png" alt="DIKW Pyramid"></p>

<h3>
<a id="approaching-the-data-banks-within-social-media" class="anchor" href="#approaching-the-data-banks-within-social-media" aria-hidden="true"><span class="octicon octicon-link"></span></a>Approaching the Data: Banks within Social Media</h3>

<p>As the data given was real, it was important to find out more about banks' part in social media before we began cleaning, organizing and gaining insights from it.  As Social Balkers, a social media research facility and consulting firm submits trends yearly, their report was used to see where the bank posts lay within the span of the greater social media.  </p>

<p>First, engagement rate typically is determined by size:</p>

<p><img src="http://www.socialbakers.com//storage/www/2014-mar/finding-er-3.png" alt="Engagement Rate by Page Size"></p>

<p>We then looked at the top banks facebook likes.  While we did not have the specific bank per bank ID, this information is useful to gage what percent of users are talking.  Through Facebook, we saw that Wells Fargo has 850,000 likes, Bank of America has 2.3 million likes. JPMorgan and Chase, and Citigroup, two other national banks do not have national facebook pages, thus their numbers are much lower -in the thousands- but have many more pages.</p>

<p>Comparing these numbers to the above graph, it is evident that each bank may be going in different directions of how much engagement they are getting. While Bank of America potentially has a low engagement of .28% and .11%, according to this graph pages like Citigroup and JPMorgan potentially has a higher engagement of .94% and .65% </p>

<p>Next, Social Balkers published these graphs to show where different industries compare to each other. In post engagement and post interaction.</p>

<p><img src="http://www.socialbakers.com//storage/www/2014-mar/finding-er-2.png" alt=""></p>

<p><img src="http://www.socialbakers.com//storage/www/2014-mar/finding-er-5.png" alt=""></p>

<p>For the top graph, an engagement is counted if it is a direct comment, like or share, and then is divided by unique page impressions (the people who have seen it).</p>

<p>Interactions are straight likes, shares, and comments. According to these graphs, Finance has a relatively mid-level engagement rate, and a low overall interaction number.  This shows that fewer people overall are seeing finance's posts, but the ones who do are 0.30% likely to comment, like or share what they do see.</p>

<p>In approaching the data, our team took these numbers slightly into consideration to give the data more context.</p>

<h2>
<a id="social-conversation-drivers" class="anchor" href="#social-conversation-drivers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Social Conversation Drivers</h2>

<p>To find the social conversation drivers, we started to index the data.  First we did this by separating the positive and negative posts.  We then asked the questions of what started the conversation and what variables, from the given data, connect.</p>

<p>Questions that connect to this are:</p>

<ul>
<li>Is there a correlation between positive/ negative remarks on different days of the week or times of the month?<br>
</li>
<li>Is there a different sort of sentiment on twitter compared to facebook?</li>
</ul>

<p>If given additional information, more questions that could be answered are:</p>

<ul>
<li>Is location a factor in and of these posts? </li>
<li>Can we geolocate, through social media, a branch that is doing really well vs poorly? </li>
<li>Does location drive a certain feeling toward one bank or another?<br>
</li>
<li>Is time is a factor on overall sentiment, topic, or substance.<br>
8 Finally, an interesting factor into all of these questions would be looking at the ages of the different posters and seeing if there is any correction between age, sentiment, topic, substance, location, and time of day.</li>
</ul>

<p>After answering these questions for a specific bank and finding numbers, comparing these to the overall internet averages would be beneficial to see where the banking world falls in social conversation compared to other industries. </p>

<h2>
<a id="code" class="anchor" href="#code" aria-hidden="true"><span class="octicon octicon-link"></span></a>Code</h2>

<div class="highlight highlight-source-r"><pre><span class="pl-c"># Load data set ('df.Rda')</span>
load(<span class="pl-s"><span class="pl-pds">"</span>df.Rda<span class="pl-pds">"</span></span>)
<span class="pl-smi">df</span><span class="pl-k">$</span><span class="pl-v">FullText</span> <span class="pl-k">=</span> as.character(<span class="pl-smi">df</span><span class="pl-k">$</span><span class="pl-smi">FullText</span>)

<span class="pl-c"># Grab just the texts, so you can load them in the Corpus</span>
<span class="pl-v">df.texts</span> <span class="pl-k">=</span> as.data.frame(<span class="pl-smi">df</span>[,ncol(<span class="pl-smi">df</span>)])
colnames(<span class="pl-smi">df.texts</span>) <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">'</span>FullText<span class="pl-pds">'</span></span>

<span class="pl-c"># Remove non-ascii characters</span>
<span class="pl-v">df.texts.clean</span> <span class="pl-k">=</span> as.data.frame(iconv(<span class="pl-smi">df.texts</span><span class="pl-k">$</span><span class="pl-smi">FullText</span>, <span class="pl-s"><span class="pl-pds">"</span>latin1<span class="pl-pds">"</span></span>, 
                                     <span class="pl-s"><span class="pl-pds">"</span>ASCII<span class="pl-pds">"</span></span>, <span class="pl-v">sub</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span><span class="pl-pds">"</span></span>))
colnames(<span class="pl-smi">df.texts.clean</span>) <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">'</span>FullText<span class="pl-pds">'</span></span>

<span class="pl-smi">df</span><span class="pl-k">$</span><span class="pl-v">FullText</span> <span class="pl-k">=</span> <span class="pl-smi">df.texts.clean</span><span class="pl-k">$</span><span class="pl-smi">FullText</span>

<span class="pl-c"># To test on 10000 samples using df.10000</span>
<span class="pl-v">idx.10000</span> <span class="pl-k">=</span> sample(<span class="pl-c1">1</span><span class="pl-k">:</span>nrow(<span class="pl-smi">df</span>),<span class="pl-c1">10000</span>)
<span class="pl-v">df.10000</span> <span class="pl-k">=</span> <span class="pl-smi">df</span>[<span class="pl-smi">idx.10000</span>,]

<span class="pl-v">df.entire</span> <span class="pl-k">=</span> <span class="pl-smi">df</span>
<span class="pl-v">df</span> <span class="pl-k">=</span> <span class="pl-smi">df.10000</span>

<span class="pl-c"># Load in corpus form using the tm library</span>
library(<span class="pl-smi">tm</span>) 
<span class="pl-smi">docs</span> <span class="pl-k">&lt;-</span> Corpus(DataframeSource(as.data.frame(<span class="pl-smi">df</span>[,<span class="pl-c1">6</span>])))   

<span class="pl-c"># Perform pre-processing</span>
<span class="pl-smi">docs</span> <span class="pl-k">&lt;-</span> tm_map(<span class="pl-smi">docs</span>, <span class="pl-smi">PlainTextDocument</span>)
<span class="pl-smi">docs</span> <span class="pl-k">&lt;-</span> tm_map(<span class="pl-smi">docs</span>, <span class="pl-smi">removePunctuation</span>)
<span class="pl-smi">docs</span> <span class="pl-k">&lt;-</span> tm_map(<span class="pl-smi">docs</span>, <span class="pl-smi">stripWhitespace</span>)
<span class="pl-smi">docs</span> <span class="pl-k">&lt;-</span> tm_map(<span class="pl-smi">docs</span>, <span class="pl-smi">removeWords</span>,c(<span class="pl-s"><span class="pl-pds">"</span>Name<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>and<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>for<span class="pl-pds">"</span></span>)) 
<span class="pl-smi">docs</span> <span class="pl-k">&lt;-</span> tm_map(<span class="pl-smi">docs</span>, <span class="pl-smi">removeWords</span>, stopwords(<span class="pl-s"><span class="pl-pds">"</span>english<span class="pl-pds">"</span></span>))
<span class="pl-smi">docs</span> <span class="pl-k">&lt;-</span> tm_map(<span class="pl-smi">docs</span>, <span class="pl-smi">removeWords</span>, stopwords(<span class="pl-v">kind</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>SMART<span class="pl-pds">"</span></span>))

save.image(<span class="pl-s"><span class="pl-pds">'</span>docs.preprocessed.Rda<span class="pl-pds">'</span></span>)
load(<span class="pl-s"><span class="pl-pds">'</span>docs.preprocessed.Rda<span class="pl-pds">'</span></span>)

<span class="pl-c"># Documents containing each bank to reference when wanting to compare different banks</span>
<span class="pl-v">bankA.idx</span> <span class="pl-k">=</span> which(sapply(<span class="pl-smi">df</span><span class="pl-k">$</span><span class="pl-smi">FullText</span>,<span class="pl-k">function</span>(<span class="pl-smi">x</span>) grepl(<span class="pl-s"><span class="pl-pds">"</span>BankA<span class="pl-pds">"</span></span>,<span class="pl-smi">x</span>)))
<span class="pl-v">bankB.idx</span> <span class="pl-k">=</span> which(sapply(<span class="pl-smi">df</span><span class="pl-k">$</span><span class="pl-smi">FullText</span>,<span class="pl-k">function</span>(<span class="pl-smi">x</span>) grepl(<span class="pl-s"><span class="pl-pds">"</span>BankB<span class="pl-pds">"</span></span>,<span class="pl-smi">x</span>)))
<span class="pl-v">bankC.idx</span> <span class="pl-k">=</span> which(sapply(<span class="pl-smi">df</span><span class="pl-k">$</span><span class="pl-smi">FullText</span>,<span class="pl-k">function</span>(<span class="pl-smi">x</span>) grepl(<span class="pl-s"><span class="pl-pds">"</span>BankC<span class="pl-pds">"</span></span>,<span class="pl-smi">x</span>)))
<span class="pl-v">bankD.idx</span> <span class="pl-k">=</span> which(sapply(<span class="pl-smi">df</span><span class="pl-k">$</span><span class="pl-smi">FullText</span>,<span class="pl-k">function</span>(<span class="pl-smi">x</span>) grepl(<span class="pl-s"><span class="pl-pds">"</span>BankD<span class="pl-pds">"</span></span>,<span class="pl-smi">x</span>)))

<span class="pl-smi">df</span><span class="pl-k">$</span><span class="pl-v">BankID</span> <span class="pl-k">=</span> vector(<span class="pl-v">mode</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>numeric<span class="pl-pds">"</span></span>, <span class="pl-v">length</span> <span class="pl-k">=</span> nrow(<span class="pl-smi">df</span>))
<span class="pl-smi">df</span><span class="pl-k">$</span><span class="pl-smi">BankID</span>[<span class="pl-smi">bankA.idx</span>] <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>BankA<span class="pl-pds">"</span></span>
<span class="pl-smi">df</span><span class="pl-k">$</span><span class="pl-smi">BankID</span>[<span class="pl-smi">bankB.idx</span>] <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>BankB<span class="pl-pds">"</span></span>
<span class="pl-smi">df</span><span class="pl-k">$</span><span class="pl-smi">BankID</span>[<span class="pl-smi">bankC.idx</span>] <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>BankC<span class="pl-pds">"</span></span>
<span class="pl-smi">df</span><span class="pl-k">$</span><span class="pl-smi">BankID</span>[<span class="pl-smi">bankD.idx</span>] <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>BankD<span class="pl-pds">"</span></span>

<span class="pl-v">bankA.docs</span> <span class="pl-k">=</span> <span class="pl-smi">docs</span>[<span class="pl-smi">bankA.idx</span>]
<span class="pl-v">bankB.docs</span> <span class="pl-k">=</span> <span class="pl-smi">docs</span>[<span class="pl-smi">bankB.idx</span>]
<span class="pl-v">bankC.docs</span> <span class="pl-k">=</span> <span class="pl-smi">docs</span>[<span class="pl-smi">bankC.idx</span>]
<span class="pl-v">bankD.docs</span> <span class="pl-k">=</span> <span class="pl-smi">docs</span>[<span class="pl-smi">bankD.idx</span>]

summary(<span class="pl-smi">docs</span>)

<span class="pl-c">## Repeat these processes for every bank</span>
<span class="pl-c">## Create document term matrix</span>
<span class="pl-smi">dtm</span> <span class="pl-k">&lt;-</span> DocumentTermMatrix(<span class="pl-smi">docs</span>[<span class="pl-smi">bankA.idx</span>])

<span class="pl-c">## Transpose this matrix</span>
<span class="pl-smi">tdm</span> <span class="pl-k">&lt;-</span> TermDocumentMatrix(<span class="pl-smi">docs</span>[<span class="pl-smi">bankA.idx</span>])

<span class="pl-c">## Remove sparse terms</span>
<span class="pl-v">dtm</span> <span class="pl-k">=</span> removeSparseTerms(<span class="pl-smi">dtm</span>, <span class="pl-c1">0.98</span>)

<span class="pl-c">## Organize terms by frequency</span>
findFreqTerms(<span class="pl-smi">dtm</span>,<span class="pl-c1">50</span>)
<span class="pl-smi">freq</span> <span class="pl-k">&lt;-</span> colSums(as.matrix(<span class="pl-smi">dtm</span>))  
<span class="pl-smi">ord</span> <span class="pl-k">&lt;-</span> order(<span class="pl-smi">freq</span>)   
<span class="pl-smi">freq</span>[head(<span class="pl-smi">ord</span>)]  
<span class="pl-smi">freq</span>[tail(<span class="pl-smi">ord</span>)]

<span class="pl-smi">wf</span> <span class="pl-k">&lt;-</span> <span class="pl-k">data.frame</span>(<span class="pl-v">word</span><span class="pl-k">=</span>names(<span class="pl-smi">freq</span>), <span class="pl-v">freq</span><span class="pl-k">=</span><span class="pl-smi">freq</span>)   
head(<span class="pl-smi">wf</span>)

<span class="pl-c">## Plot word frequencies</span>
library(<span class="pl-smi">ggplot2</span>)   
<span class="pl-smi">p</span> <span class="pl-k">&lt;-</span> ggplot(subset(<span class="pl-smi">wf</span>, <span class="pl-smi">freq</span><span class="pl-k">&gt;</span><span class="pl-c1">100</span>), aes(<span class="pl-smi">word</span>, <span class="pl-smi">freq</span>))    
<span class="pl-smi">p</span> <span class="pl-k">&lt;-</span> <span class="pl-smi">p</span> <span class="pl-k">+</span> geom_bar(<span class="pl-v">stat</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>identity<span class="pl-pds">"</span></span>)   
<span class="pl-smi">p</span> <span class="pl-k">&lt;-</span> <span class="pl-smi">p</span> <span class="pl-k">+</span> theme(<span class="pl-v">axis.text.x</span><span class="pl-k">=</span>element_text(<span class="pl-v">angle</span><span class="pl-k">=</span><span class="pl-c1">45</span>, <span class="pl-v">hjust</span><span class="pl-k">=</span><span class="pl-c1">1</span>))   
<span class="pl-smi">p</span>

<span class="pl-c">## To get a word cloud of the 100 most frequent words </span>
library(<span class="pl-smi">wordcloud</span>)
set.seed(<span class="pl-c1">142</span>)   
<span class="pl-smi">dark2</span> <span class="pl-k">&lt;-</span> brewer.pal(<span class="pl-c1">6</span>, <span class="pl-s"><span class="pl-pds">"</span>Dark2<span class="pl-pds">"</span></span>)   
wordcloud(names(<span class="pl-smi">freq</span>), <span class="pl-smi">freq</span>, <span class="pl-v">max.words</span><span class="pl-k">=</span><span class="pl-c1">25</span>, <span class="pl-v">rot.per</span><span class="pl-k">=</span><span class="pl-c1">0.2</span>, <span class="pl-v">colors</span><span class="pl-k">=</span><span class="pl-smi">dark2</span>)

<span class="pl-c"># Sentiment analysis</span>
<span class="pl-c">#Positive and Negative Sentiment Values</span>
<span class="pl-smi">pos</span> <span class="pl-k">&lt;-</span> scan(<span class="pl-s"><span class="pl-pds">'</span>positive-words.txt<span class="pl-pds">'</span></span>,<span class="pl-v">what</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>character<span class="pl-pds">'</span></span>,<span class="pl-v">comment.char</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>;<span class="pl-pds">'</span></span>)
<span class="pl-smi">neg</span> <span class="pl-k">&lt;-</span> scan(<span class="pl-s"><span class="pl-pds">'</span>negative-words.txt<span class="pl-pds">'</span></span>,<span class="pl-v">what</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>character<span class="pl-pds">'</span></span>,<span class="pl-v">comment.char</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>;<span class="pl-pds">'</span></span>)

<span class="pl-v">score.sentiment</span> <span class="pl-k">=</span> <span class="pl-k">function</span>(<span class="pl-smi">sentences</span>, <span class="pl-smi">pos.words</span>, <span class="pl-smi">neg.words</span>, <span class="pl-v">.progress</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>none<span class="pl-pds">'</span></span>)
{
  require(<span class="pl-smi">plyr</span>)
  require(<span class="pl-smi">stringr</span>)

  <span class="pl-v">scores</span> <span class="pl-k">=</span> laply(<span class="pl-smi">sentences</span>, <span class="pl-k">function</span>(<span class="pl-smi">sentence</span>, <span class="pl-smi">pos.words</span>, <span class="pl-smi">neg.words</span>) {

    <span class="pl-c"># Clean up sentences with R's regex-driven global substitute, gsub():</span>
    <span class="pl-v">sentence</span> <span class="pl-k">=</span> gsub(<span class="pl-s"><span class="pl-pds">'</span>[[:punct:]]<span class="pl-pds">'</span></span>, <span class="pl-s"><span class="pl-pds">'</span><span class="pl-pds">'</span></span>, <span class="pl-smi">sentence</span>)
    <span class="pl-v">sentence</span> <span class="pl-k">=</span> gsub(<span class="pl-s"><span class="pl-pds">'</span>[[:cntrl:]]<span class="pl-pds">'</span></span>, <span class="pl-s"><span class="pl-pds">'</span><span class="pl-pds">'</span></span>, <span class="pl-smi">sentence</span>)
    <span class="pl-v">sentence</span> <span class="pl-k">=</span> gsub(<span class="pl-s"><span class="pl-pds">'</span><span class="pl-cce">\\</span>d+<span class="pl-pds">'</span></span>, <span class="pl-s"><span class="pl-pds">'</span><span class="pl-pds">'</span></span>, <span class="pl-smi">sentence</span>)
    <span class="pl-v">sentence</span> <span class="pl-k">=</span> tolower(<span class="pl-smi">sentence</span>)

    <span class="pl-c"># Split into words. You need the stringr package</span>
    <span class="pl-v">word.list</span> <span class="pl-k">=</span> str_split(<span class="pl-smi">sentence</span>, <span class="pl-s"><span class="pl-pds">'</span><span class="pl-cce">\\</span>s+<span class="pl-pds">'</span></span>)
    <span class="pl-c"># Sometimes a list() is one level of hierarchy too much</span>
    <span class="pl-v">words</span> <span class="pl-k">=</span> unlist(<span class="pl-smi">word.list</span>)

    <span class="pl-c"># Compare words to positive &amp; negative terms</span>
    <span class="pl-v">pos.matches</span> <span class="pl-k">=</span> match(<span class="pl-smi">words</span>, <span class="pl-smi">pos.words</span>)
    <span class="pl-v">neg.matches</span> <span class="pl-k">=</span> match(<span class="pl-smi">words</span>, <span class="pl-smi">neg.words</span>)

    <span class="pl-c"># match() returns the position of the matched term or NA</span>
    <span class="pl-v">pos.matches</span> <span class="pl-k">=</span> <span class="pl-k">!</span>is.na(<span class="pl-smi">pos.matches</span>)
    <span class="pl-v">neg.matches</span> <span class="pl-k">=</span> <span class="pl-k">!</span>is.na(<span class="pl-smi">neg.matches</span>)

    <span class="pl-c"># TRUE/FALSE will be treated as 1/0 by sum():</span>
    <span class="pl-v">score</span> <span class="pl-k">=</span> sum(<span class="pl-smi">pos.matches</span>) <span class="pl-k">-</span> sum(<span class="pl-smi">neg.matches</span>)

    <span class="pl-k">return</span>(<span class="pl-smi">score</span>)
  }, <span class="pl-smi">pos.words</span>, <span class="pl-smi">neg.words</span>, <span class="pl-v">.progress</span><span class="pl-k">=</span>.<span class="pl-smi">progress</span> )

  <span class="pl-v">scores.df</span> <span class="pl-k">=</span> <span class="pl-k">data.frame</span>(<span class="pl-v">score</span><span class="pl-k">=</span><span class="pl-smi">scores</span>, <span class="pl-v">text</span><span class="pl-k">=</span><span class="pl-smi">sentences</span>)
  <span class="pl-k">return</span>(<span class="pl-smi">scores.df</span>)
}

<span class="pl-c"># Very positive and negative</span>
<span class="pl-v">df.sentiment</span> <span class="pl-k">=</span> <span class="pl-smi">df</span>[<span class="pl-smi">bankA.idx</span>,]

<span class="pl-v">scores</span> <span class="pl-k">=</span> score.sentiment(<span class="pl-smi">df.sentiment</span><span class="pl-k">$</span><span class="pl-smi">FullText</span>, <span class="pl-smi">pos</span>, <span class="pl-smi">neg</span>, <span class="pl-v">.progress</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>text<span class="pl-pds">'</span></span>)
<span class="pl-smi">scores</span><span class="pl-k">$</span><span class="pl-v">very.pos</span> <span class="pl-k">=</span> as.numeric(<span class="pl-smi">scores</span><span class="pl-k">$</span><span class="pl-smi">score</span> <span class="pl-k">&gt;</span><span class="pl-k">=</span> <span class="pl-c1">2</span>)
<span class="pl-smi">scores</span><span class="pl-k">$</span><span class="pl-v">very.neg</span> <span class="pl-k">=</span> as.numeric(<span class="pl-smi">scores</span><span class="pl-k">$</span><span class="pl-smi">score</span> <span class="pl-k">&lt;</span><span class="pl-k">=</span> <span class="pl-k">-</span><span class="pl-c1">2</span>)

<span class="pl-v">pos.tweets</span> <span class="pl-k">=</span> which(<span class="pl-smi">scores</span><span class="pl-k">$</span><span class="pl-smi">very.pos</span> <span class="pl-k">==</span> <span class="pl-c1">1</span>)
<span class="pl-v">neg.tweets</span> <span class="pl-k">=</span> which(<span class="pl-smi">scores</span><span class="pl-k">$</span><span class="pl-smi">very.neg</span> <span class="pl-k">==</span> <span class="pl-c1">1</span>)
write.csv(<span class="pl-smi">df.sentiment</span>[<span class="pl-smi">pos.tweets</span>,],<span class="pl-v">file</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>pos.texts.csv<span class="pl-pds">'</span></span>)<span class="pl-c">##creates positive</span>
write.csv(<span class="pl-smi">df.sentiment</span>[<span class="pl-smi">neg.tweets</span>,],<span class="pl-v">file</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>neg.texts.csv<span class="pl-pds">'</span></span>)<span class="pl-c">##creates negative</span>

<span class="pl-c"># Creating a classifier for pos.texts</span>
load(<span class="pl-s"><span class="pl-pds">'</span>df.Rda<span class="pl-pds">'</span></span>)
<span class="pl-smi">df</span><span class="pl-k">$</span><span class="pl-v">FullText</span> <span class="pl-k">=</span> as.character(<span class="pl-smi">df</span><span class="pl-k">$</span><span class="pl-smi">FullText</span>)
<span class="pl-v">pos.texts</span> <span class="pl-k">=</span> read.csv(<span class="pl-s"><span class="pl-pds">'</span>pos.texts.csv<span class="pl-pds">'</span></span>,<span class="pl-v">header</span><span class="pl-k">=</span><span class="pl-c1">T</span>)
<span class="pl-smi">pos.texts</span><span class="pl-k">$</span><span class="pl-v">FullText</span> <span class="pl-k">=</span> as.character(<span class="pl-smi">pos.texts</span><span class="pl-k">$</span><span class="pl-smi">FullText</span>)
colnames(<span class="pl-smi">pos.texts</span>)
<span class="pl-smi">docs</span> <span class="pl-k">&lt;-</span> Corpus(DataframeSource(as.data.frame(<span class="pl-smi">pos.texts</span>[,<span class="pl-c1">9</span>])))   
<span class="pl-smi">docs</span> <span class="pl-k">&lt;-</span> tm_map(<span class="pl-smi">docs</span>, <span class="pl-smi">PlainTextDocument</span>)

<span class="pl-smi">dtm</span> <span class="pl-k">&lt;-</span> DocumentTermMatrix(<span class="pl-smi">docs</span>)

<span class="pl-smi">tdm</span> <span class="pl-k">&lt;-</span> TermDocumentMatrix(<span class="pl-smi">docs</span>)

<span class="pl-v">m</span> <span class="pl-k">=</span> as.matrix(<span class="pl-smi">dtm</span>)

<span class="pl-v">df.classification</span> <span class="pl-k">=</span> as.data.frame(<span class="pl-smi">m</span>) <span class="pl-c">#dataframe</span>
<span class="pl-smi">df.classification</span><span class="pl-k">$</span><span class="pl-v">Relevant</span> <span class="pl-k">=</span> <span class="pl-smi">pos.texts</span><span class="pl-k">$</span><span class="pl-smi">Relevant</span>
<span class="pl-smi">df.classification</span><span class="pl-k">$</span><span class="pl-smi">Relevant</span>

<span class="pl-c"># Grow a tree</span>
library(<span class="pl-smi">rpart</span>)
<span class="pl-smi">fit</span><span class="pl-k">&lt;-</span>rpart(<span class="pl-smi">Relevant</span> <span class="pl-k">~</span> <span class="pl-smi">FullText</span> <span class="pl-k">+</span> <span class="pl-smi">BankID</span>, <span class="pl-v">data</span> <span class="pl-k">=</span> <span class="pl-smi">pos.texts</span>, <span class="pl-v">method</span> <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>class<span class="pl-pds">"</span></span>)
printcp(<span class="pl-smi">fit</span>) <span class="pl-c"># display the results </span>
plotcp(<span class="pl-smi">fit</span>) <span class="pl-c"># visualize cross-validation results </span>
summary(<span class="pl-smi">fit</span>) <span class="pl-c"># detailed summary of splits</span>

<span class="pl-c"># Creating a classifier for neg.texts</span>
load(<span class="pl-s"><span class="pl-pds">'</span>df.Rda<span class="pl-pds">'</span></span>)
<span class="pl-smi">df</span><span class="pl-k">$</span><span class="pl-v">FullText</span> <span class="pl-k">=</span> as.character(<span class="pl-smi">df</span><span class="pl-k">$</span><span class="pl-smi">FullText</span>)
<span class="pl-v">neg.texts</span> <span class="pl-k">=</span> read.csv(<span class="pl-s"><span class="pl-pds">'</span>neg.texts.csv<span class="pl-pds">'</span></span>,<span class="pl-v">header</span><span class="pl-k">=</span><span class="pl-c1">T</span>)
<span class="pl-smi">neg.texts</span><span class="pl-k">$</span><span class="pl-v">FullText</span> <span class="pl-k">=</span> as.character(<span class="pl-smi">neg.texts.texts</span><span class="pl-k">$</span><span class="pl-smi">FullText</span>)
colnames(<span class="pl-smi">neg.texts</span>)
<span class="pl-smi">docs</span> <span class="pl-k">&lt;-</span> Corpus(DataframeSource(as.data.frame(<span class="pl-smi">neg.texts</span>[,<span class="pl-c1">9</span>])))   
<span class="pl-smi">docs</span> <span class="pl-k">&lt;-</span> tm_map(<span class="pl-smi">docs</span>, <span class="pl-smi">PlainTextDocument</span>)

<span class="pl-smi">dtm</span> <span class="pl-k">&lt;-</span> DocumentTermMatrix(<span class="pl-smi">docs</span>)

<span class="pl-smi">tdm</span> <span class="pl-k">&lt;-</span> TermDocumentMatrix(<span class="pl-smi">docs</span>)

<span class="pl-v">m</span> <span class="pl-k">=</span> as.matrix(<span class="pl-smi">dtm</span>)

<span class="pl-v">df.classification</span> <span class="pl-k">=</span> as.data.frame(<span class="pl-smi">m</span>) <span class="pl-c">#dataframe</span>
<span class="pl-smi">df.classification</span><span class="pl-k">$</span><span class="pl-v">Relevant</span> <span class="pl-k">=</span> <span class="pl-smi">neg.texts</span><span class="pl-k">$</span><span class="pl-smi">Relevant</span>
<span class="pl-smi">df.classification</span><span class="pl-k">$</span><span class="pl-smi">Relevant</span>
colnames(<span class="pl-smi">neg.texts</span>)

<span class="pl-c"># Grow a tree</span>
library(<span class="pl-smi">rpart</span>)
<span class="pl-smi">fit</span><span class="pl-k">&lt;-</span>rpart(<span class="pl-smi">Relevant</span> <span class="pl-k">~</span> <span class="pl-smi">FullText</span> <span class="pl-k">+</span> <span class="pl-smi">BankID</span>, <span class="pl-v">data</span> <span class="pl-k">=</span> <span class="pl-smi">neg.texts</span>, <span class="pl-v">method</span> <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>class<span class="pl-pds">"</span></span>)
printcp(<span class="pl-smi">fit</span>) <span class="pl-c"># display the results </span>
plotcp(<span class="pl-smi">fit</span>) <span class="pl-c"># visualize cross-validation results </span>
summary(<span class="pl-smi">fit</span>) <span class="pl-c"># detailed summary of splits</span>

<span class="pl-c">## Cluster Dendrogram</span>
<span class="pl-c"># Useless words</span>
<span class="pl-smi">docs</span> <span class="pl-k">&lt;-</span> tm_map(<span class="pl-smi">docs</span>, <span class="pl-smi">removeWords</span>,c(<span class="pl-s"><span class="pl-pds">"</span>Name<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>and<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>for<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>name<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>this<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>are<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>from<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>just<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>get<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>ret_twit<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>name_resp<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>twit_hndl<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>twit_handl:<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>twit_hndl_banka<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>ly/<span class="pl-pds">"</span></span>)) 
<span class="pl-smi">docs</span> <span class="pl-k">&lt;-</span> tm_map(<span class="pl-smi">docs</span>, <span class="pl-smi">removeWords</span>, stopwords(<span class="pl-s"><span class="pl-pds">"</span>english<span class="pl-pds">"</span></span>))
<span class="pl-smi">docs</span> <span class="pl-k">&lt;-</span> tm_map(<span class="pl-smi">docs</span>, <span class="pl-smi">removeWords</span>, stopwords(<span class="pl-v">kind</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>SMART<span class="pl-pds">"</span></span>))

<span class="pl-smi">dtm</span> <span class="pl-k">&lt;-</span> DocumentTermMatrix(<span class="pl-smi">docs</span>)
<span class="pl-smi">tdm</span> <span class="pl-k">&lt;-</span> TermDocumentMatrix(<span class="pl-smi">docs</span>)

<span class="pl-c"># remove sparse terms</span>
<span class="pl-smi">tdm2</span> <span class="pl-k">&lt;-</span> removeSparseTerms(<span class="pl-smi">tdm</span>, <span class="pl-v">sparse</span> <span class="pl-k">=</span> <span class="pl-c1">0.97</span>)
<span class="pl-smi">m2</span> <span class="pl-k">&lt;-</span> as.matrix(<span class="pl-smi">tdm2</span>)
<span class="pl-c"># cluster terms</span>
<span class="pl-smi">distMatrix</span> <span class="pl-k">&lt;-</span> dist(scale(<span class="pl-smi">m2</span>))
<span class="pl-smi">fit</span> <span class="pl-k">&lt;-</span> hclust(<span class="pl-smi">distMatrix</span>, <span class="pl-v">method</span> <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>ward.D<span class="pl-pds">"</span></span>)

plot(<span class="pl-smi">fit</span>)
rect.hclust(<span class="pl-smi">fit</span>, <span class="pl-v">k</span> <span class="pl-k">=</span> <span class="pl-c1">6</span>)

<span class="pl-c">##Cluster Dendrogram</span>
<span class="pl-smi">docs</span> <span class="pl-k">&lt;-</span> tm_map(<span class="pl-smi">docs</span>, <span class="pl-smi">removeWords</span>,c(<span class="pl-s"><span class="pl-pds">"</span>Name<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>and<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>for<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>name<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>this<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>are<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>from<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>just<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>get<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>ret_twit<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>name_resp<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>twit_hndl<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>twit_handl:<span class="pl-pds">"</span></span>, <span class="pl-s"><span class="pl-pds">"</span>twit_hndl_banka<span class="pl-pds">"</span></span>,<span class="pl-s"><span class="pl-pds">"</span>ly/<span class="pl-pds">"</span></span>)) 
<span class="pl-smi">docs</span> <span class="pl-k">&lt;-</span> tm_map(<span class="pl-smi">docs</span>, <span class="pl-smi">removeWords</span>, stopwords(<span class="pl-s"><span class="pl-pds">"</span>english<span class="pl-pds">"</span></span>))
<span class="pl-smi">docs</span> <span class="pl-k">&lt;-</span> tm_map(<span class="pl-smi">docs</span>, <span class="pl-smi">removeWords</span>, stopwords(<span class="pl-v">kind</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>SMART<span class="pl-pds">"</span></span>))

<span class="pl-smi">dtm</span> <span class="pl-k">&lt;-</span> DocumentTermMatrix(<span class="pl-smi">docs</span>)
<span class="pl-smi">tdm</span> <span class="pl-k">&lt;-</span> TermDocumentMatrix(<span class="pl-smi">docs</span>)

<span class="pl-c">##############</span></pre></div>

<h2>
<a id="topics-discussed" class="anchor" href="#topics-discussed" aria-hidden="true"><span class="octicon octicon-link"></span></a>Topics Discussed</h2>

<p>Top Topics discussed: </p>

<ul>
<li>Customer Service</li>
<li>ATM</li>
<li>Security</li>
<li>Privacy</li>
<li>App</li>
<li>Account Access</li>
<li>Account Request</li>
<li>Card</li>
<li>Customer Loss</li>
<li>Fees</li>
</ul>

<p>The three main topics discussed were customer service, fees and ATM.
ATM</p>

<ul>
<li>Customer dislikes interacting with ATM due to lack of privacy, security, and customer service. </li>
<li>Card often goes unread at ATM’s and stores.</li>
</ul>

<p>Customer Service </p>

<ul>
<li>Customer is leaving bank because of rude bankers.</li>
<li>Customer Service was not helpful in explaining fees.</li>
<li>Customer dislikes ATM due to lack of customer service</li>
</ul>

<p>Fees</p>

<ul>
<li>Customer  leaving bank  because of excessive fees.</li>
<li>Customer has found more favorable conditions  at different bank .</li>
<li>Too many fees withdrawing money at ATMs </li>
</ul>

<h2>
<a id="insights" class="anchor" href="#insights" aria-hidden="true"><span class="octicon octicon-link"></span></a>Insights</h2>

<h3>
<a id="classification-to-find-comments-of-substance" class="anchor" href="#classification-to-find-comments-of-substance" aria-hidden="true"><span class="octicon octicon-link"></span></a>Classification to find comments of substance.</h3>

<p>We started creating a code that would classify the tweets and posts into spam or real.  We did this for the positive and negative posts separately to see the differences and similarities between what can be responded to, both on social media, as well as from a business stand-point.  In doing so, and testing parts of the data, we created two decision trees that showed what key words (aside from names, twitter handle, and the bank name)<br>
Our suggestion would be to have the people responding to tweets/ posts putting if they are spam or not so the algorithm becomes more sophisticated.  In doing so, when looking at the tweets later on, the data will be better minned if there was an algorithm that took out nonrelevant or ‘spam’ tweets and posts. </p>

<table>
<thead>
<tr>
<th>Positive Classification Relevancy</th>
<th>Negative Classification Relevancy</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="http://i.imgur.com/L8f24Az.png" alt="Positive Classification"></td>
<td><img src="http://i.imgur.com/ul8pehC.png" alt="Negative Classification "></td>
</tr>
</tbody>
</table>

<p>According to our tree diagram, this diagram displays the positive and negative relevant words. This is significant because here we are able to plot the relevancy of positive sentiments as it shows overall bank's performance within the industry. When banks are acknowledged for their service they receive positive statements within a .5 relevancy.  With a less than .5 relevancy, the post may not be worth the time of the analyzer on the next social media analysis.  This can be helpful to decrease wasted space and a faster analysis process because there will be a significant amount less posts and tweets.  In our research, out of 301 posts, only 137 were relevant. </p>

<h3>
<a id="frequent-topics-and-related-items" class="anchor" href="#frequent-topics-and-related-items" aria-hidden="true"><span class="octicon octicon-link"></span></a>Frequent Topics and Related Items</h3>

<p><img src="http://i.imgur.com/roYZq10.png" alt="Cluster Dendrogram">
This dedrogram, created by a member or my team shows some of the hierarchical pairings of words commonly found together.  This dendrogram makes come interesting connections:</p>

<ul>
<li>At the smaller connections, money and phone are linked suggestion mobile app use, or customer service ties</li>
<li>BankD is commonly linked to url links(represented by 'internet')</li>
<li>BankA is clustered with twitter handles which shows that there was a high percent of the posts tweets about bank A</li>
<li>Program, full, learn, and small are all linked.  As these have less to do with banking topics, with more time, we would dig into these key-words to see what the conversation was about</li>
<li>Account and nameresp are connected which means that there are frequent conversations on facebook about users' accounts.</li>
<li>Today is closely clustered with business and street.  This may imply that the users had an urgency in posting that day and that the experience was in person- a business or a street addressed.  This can either mean a walk in bank or ATM location. </li>
</ul>

<p>Regardless of sentiment value, the four banks were surveyed separately and together to find the most common topics for each, represented in a word cloud.</p>

<p><img src="http://i.imgur.com/CnUjkLu.png" alt="Word Cloud">  <img src="http://i.imgur.com/0yhd2Wy.png" alt="Histogram"></p>

<p>Above shows the frequent words in the entire dataset.  This can be compared to the most common words for the top four banks shown below. </p>

<table>
<thead>
<tr>
<th>Bank A</th>
<th>Bank B</th>
<th>Bank C</th>
<th>Bank D</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="http://i.imgur.com/dM6is8e.png" alt="Bank A"></td>
<td><img src="http://i.imgur.com/JtEpnSh.png" alt="Bank B"></td>
<td><img src="http://i.imgur.com/JO61606.png" alt="Bank C"></td>
<td><img src="http://i.imgur.com/0IhqiY0.png" alt="Bank D"></td>
</tr>
</tbody>
</table>

<p>Bank A included service, account, money, check, help, customer service, and phone.  These words show that most of the posts do have to do with the bank and the conversations are about aspects of the bank.  Also, it is of note that bankB is commonly mentioned so bankA and bankB may be close competitors.  With a high frequency of the twitter handle, it shows that many of the interactions were on twitter.</p>

<p>Bank B top comments included account, team, credit, share, service, customer, thanks.
Through the cop words in bankB’s tweets and posts, it looks like they may have a higher sentiment value than some of the other banks.  There was not a single topic that was predominantly present.  The inclusion of Chicago shows that there may be a hub or large bank present in the city. </p>

<p>Bank C top comments included credit, pay, global, financial, banking, rating, credit, and business.
Through the top topics for Bank C, it is evident that their connections are the most involved with banking.  The bank must not be as involved with the community as other banks.  </p>

<p>Bank D shows a high connection to the community around them.  Whether it is Bank D that exists in another form other than the bank, it seems that bank D sponsors many things. With words like apply, program, and business, the bank shows more engagement words than the other banks.</p>

<h2>
<a id="conclusion" class="anchor" href="#conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion</h2>

<p>Social Media is not only a place to talk to friends or a marketing ploy.  It is a place where our society continues.  Thus, it is important to understand its importance and what people on it are saying about companies.  Through this analysis we have seen many things.  It is probable that all banks have a range of polarity in sentiment.  This report shows suggestions in how it can be analyzed in the future:</p>

<ul>
<li>While going through tweets and posts declaring their relevancy to be able to use classification in the future</li>
<li>Seeing the top topics discussed using word frequency</li>
<li>Using clustering to determine the closeness of different words. </li>
</ul>

<p>With the results we processed, we saw a range of engagement and interactions in comments.  With more information like likes and shares, we would be able to better compare our results with the research at the beginning on engagement and interactions.  Overall, our results show that all banks seem to have a range of sentiment about all of the topics that we outlined.  </p>

<p>In conclusion, with the data given to us by Wells Fargo. We were able to manipulate it to figure out the social drivers that a bank may obtain from their customers through the use of social media. Using R and data mining we were able to classify and determine the relevancy using sentiment analysis and the proper algorithms and arithmetic to provide the outcomes of this data competition. As social media continues to progress and is widely used, we can most likely count on them to retrieve the data we need to figure out the likely causes of customer service in an industry.</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/rachel-feldman/wellsfargosocialmedia">Wells Fargo Social Media Analytic Challenge</a> is maintained by <a href="https://github.com/rachel-feldman">rachel-feldman</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
